{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "DB_FAISS_PATH = \"vectorstores/faiss_db\"\n",
    "\n",
    "# Create a Vector DB\n",
    "def create_vector_db(data):\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents =  loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "    model_kwargs = {\n",
    "        'device': 'cpu',\n",
    "    })\n",
    "    db = FAISS.from_documents(chunks, embeddings)\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = create_vector_db(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = \"\"\"\n",
    "Use the following prompt to retrieve information from the database:\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt():\n",
    "    prompt = PromptTemplate(template=custom_prompt, input_variables=[\"question\", \"context\"])\n",
    "    return prompt\n",
    "\n",
    "def load_llm():\n",
    "    llm = CTransformers(model=\"model/llama-2-7b-chat.ggmlv3.q8_0.bin\", \n",
    "                        model_type=\"llama\",\n",
    "                        max_new_tokens=512,\n",
    "                        tempterature=0.5,)\n",
    "    return llm\n",
    "\n",
    "def retrieve_answer(llm, prompt, db):\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                           chain_type=\"stuff\",\n",
    "                                           retriever = db.as_retriever(search_kwargs={'k': 2}),\n",
    "                                           return_source_documents=True,\n",
    "                                           chain_type_kwargs={'prompt': prompt}\n",
    "                                           )\n",
    "    return qa_chain\n",
    "\n",
    "def qa_bot():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "                                        model_kwargs = {\n",
    "                                             'device': 'cpu',\n",
    "                                        })\n",
    "    db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    llm = load_llm()\n",
    "    qa_prompt = set_custom_prompt()\n",
    "    qa = retrieve_answer(llm, qa_prompt, db)\n",
    "    return qa\n",
    "\n",
    "def final_result(query):\n",
    "    result = qa_bot()\n",
    "    response = result({'question': query})\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set custom prompt\n",
    "prompt = set_custom_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm =load_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_answer(llm=llm, prompt=prompt, db=db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/jk/x7843gt92gn5x1xqygl255_w0000gn/T/ipykernel_88519/2114493568.py\", line 5, in <module>\n",
      "    retrieve_answer(llm=llm, prompt=prompt, db=db)\n",
      "  File \"/var/folders/jk/x7843gt92gn5x1xqygl255_w0000gn/T/ipykernel_88519/2114493568.py\", line 2, in retrieve_answer\n",
      "    qa = RetrievalQA(retriever=db.as_retriever(), combine_documents_chain={'llm': llm, 'prompt': prompt}, combine_documents_chain_kwargs={'max_new_tokens': 512, 'temperature': 0.5, 'top_p': 0.95, 'max_tokens': 512, 'num_return_sequences': 1, 'stop_sequences': ['\\n']}, combine_documents_chain_type='llm', combine_documents_chain_type_kwargs={'model': 'gpt2', 'max_new_tokens': 512, 'temperature': 0.5, 'top_p': 0.95, 'max_tokens': 512, 'num_return_sequences': 1, 'stop_sequences': ['\\n']}, retriever_kwargs={'k': 2, 'search_kwargs': {'k': 2}}, retriever_type='stuff', retriever_type_kwargs={'prompt': 'Retrieve the answer to the following question: {question} from the database.'})\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/langchain/load/serializable.py\", line 74, in __init__\n",
      "  File \"pydantic/main.py\", line 341, in pydantic.main.BaseModel.__init__\n",
      "pydantic.error_wrappers.ValidationError: 8 validation errors for RetrievalQA\n",
      "combine_documents_chain\n",
      "  Can't instantiate abstract class BaseCombineDocumentsChain with abstract methods acombine_docs, combine_docs (type=type_error)\n",
      "retriever\n",
      "  Can't instantiate abstract class BaseRetriever with abstract methods _aget_relevant_documents, _get_relevant_documents (type=type_error)\n",
      "combine_documents_chain_kwargs\n",
      "  extra fields not permitted (type=value_error.extra)\n",
      "combine_documents_chain_type\n",
      "  extra fields not permitted (type=value_error.extra)\n",
      "combine_documents_chain_type_kwargs\n",
      "  extra fields not permitted (type=value_error.extra)\n",
      "retriever_kwargs\n",
      "  extra fields not permitted (type=value_error.extra)\n",
      "retriever_type\n",
      "  extra fields not permitted (type=value_error.extra)\n",
      "retriever_type_kwargs\n",
      "  extra fields not permitted (type=value_error.extra)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/zaineelmithani/anaconda3/envs/medicalChatbot/lib/python3.8/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "def retrieve_answer(llm, prompt, db):\n",
    "    qa = RetrievalQA()\n",
    "    return qa\n",
    "\n",
    "retrieve_answer(llm=llm, prompt=prompt, db=db)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medicalChatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
